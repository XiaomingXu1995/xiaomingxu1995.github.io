<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Min Zhao (ËµµÊïè)</title>

  <meta name="author" content="Min Zhao">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/avatar.jpg">


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Min Zhao (ËµµÊïè)</name>
                  </p>

                  <p align="justify">
                    My name is Min Zhao. I am a postdoctoral researcher at Tsinghua University, working under the supervision of Prof. <a href="http://ml.cs.tsinghua.edu.cn/~jun/index.shtml" target="_blank" rel="noopener"> Jun Zhu</a>. I have collaborated closely with Prof. <a href="https://zhenxuan00.github.io/" target="_blank" rel="noopener"> Chongxuan Li</a>.
My research interests include AIGC, especially for image/video/3D generation with diffusion models. Feel free to connect with me for research collaborations or discussions related to the above topics.
                    
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:gracezhao1997@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ExIZrLAAAAAJ&hl=zh-CN&oi=sra">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/gracezhao1997"> GitHub </a> &nbsp/&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <table align="center" border="0" cellpadding="10" cellspacing="0" width="100%">
            <tbody>
              <tr>
                <td valign="middle" width="100%">
                  <heading>News </heading>
                </td>
              </tr>
            </tbody>
          </table>


          <ul>
               <li><span style="color: gray" size="6px">[2024.05]</span>
              üéâ Two papers: <a href="https://arxiv.org/abs/2401.13601"><strong>MM-LLMs</strong></a> and <a href=""><strong>WT\&WF</strong></a> are accepted by <strong>ACL2024</strong>. See you in Bangkok!
            </li>

            <li><span style="color: gray" size="6px">[2024.05]</span>
              üéâ One paper: <a href=""><strong>PPAE</strong></a> is accepted by <strong>ICML2024</strong>.
            </li>

               <li><span style="color: gray" size="6px">[2024.02]</span>
              üì£ Stay updated on our latest review articles regarding <a href="https://arxiv.org/abs/2401.13601"><strong>MM-LLMs</strong></a>.
            </li>

            <li><span style="color: gray" size="6px">[2023.10]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2301.10292"><strong>SPN-GA</strong></a> is accepted by <strong>Machine Intelligence Research (MIR)</strong>.
            </li>

            <li><span style="color: gray" size="6px">[2023.10]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2310.14541"><strong>CPFD</strong></a> is accepted by <strong>EMNLP2023</strong> Main Conference as a Long Paper. See you in Singapore!
            </li>


            <li><span style="color: gray" size="6px">[2023.09]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2309.14078"><strong>ODE-RNN4RL</strong></a> is accepted by <strong>NeurIPS2023</strong>.
            </li>


            <li><span style="color: gray" size="6px">[2023.08]</span>
              üéâ One paper: <a href="https://dl.acm.org/doi/abs/10.1145/3583780.3615075"><strong>RDP</strong></a> is accepted by <strong>CIKM2023</strong> as a Long Paper. (<span style="color: red" size="6px"><strong>Oral</strong></span>)
            </li>


            <li><span style="color: gray" size="6px">[2023.05]</span>
              üéâ One paper: <a href="https://aclanthology.org/2023.acl-long.408/"><strong>DualGATs</strong></a> is accepted by <strong>ACL2023</strong> Main Conference as a Long Paper.
            </li>


            <li><span style="color: gray" size="6px">[2023.04]</span>
              üéâ One paper: <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591970"><strong>DLD</strong></a> is accepted by <strong>SIGIR2023</strong>.
            </li>


            <li><span style="color: gray" size="6px">[2023.02]</span>
              üéâ One paper: <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.html"><strong>FISS</strong></a> is accepted by <strong>CVPR2023</strong>.
            </li>


            <li><span style="color: gray" size="6px">[2023.01]</span>
              üéâ One paper: <a href="https://ieeexplore.ieee.org/abstract/document/10219015"><strong>SAMGN</strong></a> is accepted by <strong>IEEE TMM</strong>.
            </li>
         
  
          </ul>



          <table align="center" border="0" cellpadding="10" cellspacing="0" width="100%">
            <tbody>
              <tr>
                <td valign="middle" width="100%">
                  <heading>Research Overview </heading>
                </td>
              </tr>
            </tbody>
          </table>

  
          <ul>
              <li>
                  <strong>Emotion Analysis in Conversational Systems:</strong>
                  <a href="https://aclanthology.org/2020.coling-main.392/">[COLING2020]</a>
                  , <a href="https://aclanthology.org/2022.coling-1.588/">[COLING2022]</a>
                  , <a href="https://ieeexplore.ieee.org/abstract/document/10219015">[IEEE TMM]</a>
                  , <a href="https://aclanthology.org/2023.acl-long.408/">[ACL2023]</a>
              </li>

               <li>
              <strong>Brain¬≠-inspired Intelligence, Spiking Neural Networks, Reinforcement Learning:</strong>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19879">[AAAI2022]</a>
              , <a href="https://arxiv.org/abs/2204.07050">[IJCAI2022]</a>,
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25081">[AAAI2023]</a>,
              <a href="https://kns.cnki.net/kcms2/article/abstract?v=Eo9-C_M6tLkCF-ZWihqs-gCIpgHnxW86iit3wSHNIltOC7iK2Y94zRZ2zS7RvDtEOTn1_qMsCgFzehh8azYcoiZlW30SRWwpSVR8SD-HCNfuKEqQo27Ma6zQVKLtLAqWXXyhin-4VhkB27qAy_cFmQ==&uniplatform=NZKPT&language=CHS">[‰∫∫Â∑•Êô∫ËÉΩ]</a>
              , <a href="https://arxiv.org/abs/2301.10292">[MIR2023]</a>,
              <a href="https://arxiv.org/abs/2308.02557">[Preprint]</a>
              , <a href="https://arxiv.org/pdf/2309.14078">[NeurIPS2023]</a>
          </li>

              <li>
                <strong>Continual Learning in Information Extraction:</strong>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.html">[CVPR2023]</a>
                , <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591970">[SIGIR2023]</a>
                , <a href="https://dl.acm.org/doi/abs/10.1145/3583780.3615075">[CIKM2023]</a>
                , <a href="https://arxiv.org/abs/2310.14541">[EMNLP2023]</a>,
                <a href="">[ACL2024]</a>
            </li>

  

          <li>
            <strong>Multi-Modal Learning, Multi-Modal Large Language Models:</strong>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547776">[ACM MM2022]</a>
            , <a href="https://link.springer.com/article/10.1007/s11633-022-1369-5">[MIR2022]</a>, <a href="">[ICML2024]</a>, <a href="https://arxiv.org/abs/2401.13601">[ACL2024]</a>
        </li>
        
          

            </ul>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p align="justify"><strong>* denotes equal contribution.</strong></p>
                </td>
         
              </tr>
            </tbody>
      
          </table>
         

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/posecrafter.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Poses
                    </papertitle>
                  </a>
                  <br /><br />
                  Yong Zhong,
                  <strong>Min Zhao</strong>,
                  Zebin You,
                  Xiaofeng Yu,
                  Changwang Zhang,
                  Chongxuan Li
                  <br /><br />
                  <strong>Arixv</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2405.14582">Paper</a>&nbsp/&nbsp
                  <a href="https://ml-gsai.github.io/PoseCrafter-demo/">Website</a>
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/vidu.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models
                    </papertitle>
                  </a>
                  <br /><br />
                  Fan Bao,
                  Chendong Xiang,
                  Gang Yue,
                  Guande He,
                  Hongzhou Zhu,
                  Kaiwen Zheng,
                  <strong>Min Zhao</strong>,
                  Shilong Liu,
                  Yaole Wang,
                  Jun Zhu
                  <br /><br />
                  <strong>Arixv</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2405.04233">Paper</a>&nbsp/&nbsp
                  <a href="https://www.shengshu-ai.com/vidu">Website</a>
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/controlvideo.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Controlvideo: Adding conditional control for one shot text-to-video editing
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Min Zhao</strong>,
                  Rongzhen Wang,
                  Fan Bao,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>Arixv</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2305.17098">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/thu-ml/controlvideo">Code</a>&nbsp/&nbsp
                  <a href="https://ml.cs.tsinghua.edu.cn/controlvideo/">Website</a>&nbsp/&nbsp
                </td>
              </tr>


            
              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/EEGSDE.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Equivariant Energy-Guided SDE for Inverse Molecular Design
                    </papertitle>
                  </a>
                  <br /><br />
                  Fan Bao,
                  <strong>Min Zhao</strong>,
                  Zhongkai Hao,
                  Peiyao Li,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>ICLR 2023</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2209.15408">Paper</a>
                  <a href="https://github.com/gracezhao1997/EEGSDE">Code</a>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/16.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle> EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations
                    </papertitle>
                  </a>
                  <br /><br />
                  Qingyu Wang*,
                  <strong>Duzhen Zhang*</strong>,
                  Tielin Zhang,
                  Bo Xu
                  <br /><br />
                  <strong>Preprint</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2207.06635">Paper</a>
                  <a href="https://github.com/ML-GSAI/EGSDE">Code</a>
                </td>
              </tr>

          </table>


          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Services</heading>
                <p align="justify"><strong>Conference Reviewers</strong>: AAAI2023, IJCAI2023, ACL2023, EMNLP2023, AAAI2024, CVPR2024, IJCAI2024, ACL2024, EMNLP2024</p>
                <p align="justify"><strong>Journal Reviewers</strong>: Computer Speech & Language</p>
              </td>
            </tr>
          </tbody>
        </table>



          <br />
<br />
<a href="https://visitorbadge.io/status?path=bladedancer957.github.io">
  <img
    src="https://api.visitorbadge.io/api/visitors?path=bladedancer957.github.io&labelColor=%23dce775&countColor=%23263759&style=flat" />
</a>
<p align="left">
  <font size="2"><a href="https://people.eecs.berkeley.edu/~barron/">website template</a> </font>
</p>


        </td>
      </tr>


  </table>
</body>

</html>
